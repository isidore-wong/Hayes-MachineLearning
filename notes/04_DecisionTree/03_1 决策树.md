## 决策树
---

### 1、决策树的基本内容
**1.1 含义：**
决策树是一种基本的分类与回归方法，是一种自上而下，对样本数据进行树形分类的过程。

**决策树的学习目的：**
为了产生一颗泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的“分而治之”的策略。

**决策树的特点：**
- 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
- 缺点：对未知的测试数据未必有好的分类、泛化能力，即可能发生过拟合现象，此时可采用剪枝或随机森林
- 适用数据类型：数值型和标称型

**1.2 决策树的组成：**
- 根结点：所有的样本数据
- 内部结点：样本数据中的某一个特征或属性
- 有向边：对应特征/属性可能对应的结果
- 叶结点：决策的结果，代表类别

**决策树生成过程中需注意的问题：** 
- 数据如何分割
~~~~
1. 分裂属性的数据类型分为离散型和连续性两种情况
2. 对于离散型的数据，按照属性值进行分裂，每个属性值对应一个分裂节点
3. 对于连续性属性，一般性的做法是对数据按照该属性进行排序，再将数据分成若干区间(对连续性属性进行分箱操作)
~~~~
- 如何选择分裂的属性
~~~~
决策树采用贪婪思想进行分裂，即选择可以得到最优分裂结果的属性进行分裂。常采用信息增益和信息增益率来进行分裂属性的选择，下面将在特征选择章节详细介绍。
~~~~
- 什么时候停止分裂
~~~~
1. 最小节点数
2. 熵或者基尼值小于阀值
3. 决策树的深度达到指定的条件
4. 所有特征已经使用完毕，不能继续进行分裂
~~~~
---

### 2、决策树的生过程 
1. 特征选择
2. 树的构建
3. 树的剪枝

**2.1 决策树的构建：**
- 步骤1：将所有的数据看成是一个结点，进入步骤2
- 步骤2：从所有的数据特征中挑选一个数据特征对结点进行分割，进入步骤3
- 步骤3：生成若干孩子结点，对每一个孩子结点进行判断，如果满足停止分裂的条件，进入步骤4；否则，进入步骤2
- 步骤4：设置该结点是子结点，其输出的结果为该结点数量占比最大的类别

**==2.1.1 决策树学习的基本算法==：**
- ==输入==：

训练集D：
```math
D = \{(x_1, y_1), (x_2, y_2),...,(x_n, y_n)\}
```
属性集A：
```math
A = \{a_1, a_2,...,a_d \}
```

- ==输出==：以node为根结点的一颗决策树
- ==过程==：
~~~~
1. 生成结点node
2. if D中样本完全属于同一类别C then 
      将node标记为C类叶结点 return 
   end if
3. if A=空集 OR D中样本在A上取值相同 then 
      将node标记为叶结点，其类别标记为D中样本数最多的类 return
   end if
4. 从A中选择最优划分属性a_*
   for a_*的每一个值a_*^v  do
       为node生成一个分支；令D_v表示D中在a_*上取值为a_*^v的样本子集；
       if D_v为空  then
          将分支结点标记为叶结点，其类别标记为D中样本最多的类  return
       else 
          以 TreeGenerate(D_v, A \ {a_*})为分支结点
       end if
   end for
~~~~


**2.2 特征选择：**
1. **熵**：熵hi度量样本不纯度的指标，描述了数据的混乱程度，熵越大，混乱程度越高，也就是纯度越低；反之，熵越小，混乱程度越低，纯度越高；熵是度量信息的期望值。
- 信息：其中p(Xi)表示选择该分类的概率
```math
l(x) = -\log_2{p(x_i)}
```
- 熵(entropy)：
```math
H(x) = -\sum_{i=1}^{n}p(x_i)\log_2{p(x_i)}
```
- 条件熵(conditional entropy)
~~~~
条件熵H(Y∣X) 表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵 H(Y|X)，定义X给定条件下Y的条件概率分布的熵对X的数学期望：
~~~~
```math
H(Y|X)=\sum_i^n{p_i} H(Y|X=x_i)

其中p_i=P(X=x_i), i=1,2,...,n
```

2. **信息增益:(ID3)**
~~~~
1. 用信息增益是相对于特征而言的，表示分裂前后,根的数据复杂度和分裂节点数据复杂度的变化值，信息增益越大，分裂后的复杂度减小得越多，分类的效果越明显
2. 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差
~~~~
```math
information\ \ gain : 
g(D, A) = H(D) - H(D|A)
```

==**ID3算法**==
- ID3算法的核心：在决策树各结点上应用信息增益准则选择特征，选择信息增益最大的特征进行分裂，递归地构造决策树（算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索））。
- ID3 算法是建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。
- ID3算法的步骤：
~~~~
1. 初始化特征集合和数据集合
2. 计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点
3. 更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）
4. 重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点
~~~~
- 分类标准：信息增益，具体公式见上式--信息增益
- ID3算法的缺点：
~~~~
1. ID3 没有剪枝策略，容易过拟合
2. 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于1
3. 只能用于处理离散分布的特征
4. 没有考虑缺失值。
~~~~
---

3. **信息增益率:(C4.5)**
~~~~
1. 信息增益率是在信息增益的基础上除以分裂节点数据量的信息增益,是为了解决信息增益的一个不可避免的缺点：倾向选择分支比较多的属性进行分裂
2. 特征A对训练数据集D的信息增益比gR(D,A)定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵HA(D)之比
~~~~
```math
information\ \ gain\ \ ratio: 
g_R(D, A) = \frac{g(D,A)}{H_A(D)}
```

==**C4.5算法**==
- C4.5 算法最大的特点是克服了ID3对特征数目的偏重这一缺点，引入信息增益率来作为分类标准
- C4.5的思想： C4.5相对于ID3的缺点对应有以下改进方式
~~~~
1. 引入悲观剪枝策略进行后剪枝
2. 引入信息增益率作为划分标准
3. 将连续特征离散化，假设n个样本的连续特征A有m个取值，C4.5将其排序并取相邻两样本值的平均数共m-1个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点
4. 对于缺失值的处理可以分为两个子问题：
- 在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）
- 选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里） 
- 针对问题一，C4.5的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算
- 针对问题二，C4.5的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中
~~~~
- 信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此C4.5并不是直接用增益率最大的特征进行划分，而是使用一个启发式方法：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的
---

4. **基尼值：(CART)**
~~~~
1. 基尼值与熵同样都是表示数据的混乱程度的，基尼值越大，数据越不纯，混乱程度越高
2. 分类问题中用基尼指数选择最优特征，同时决定该特征的最优二值切分点
3. 假设有K个类，样本点属于第k类的概率为pk，则概率分布的基尼指数定义为
~~~~
```math
gini\ \ index:
Gini(D)=\sum_{k=1}^{K}p_k(1-p_k))=1-\sum_{k=1}^{K}p_k^2
```
---

==**CART算法**==
- ID3和C4.5虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率
- CART算法的思想：CART包含的基本过程有分裂，剪枝和树选择
~~~~
1. 分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART没有停止准则，会一直生长下去
2. 剪枝：采用代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树
3. 树选择：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）
~~~~
- CART在C4.5的基础上进行了很多提升
~~~~
1. C4.5为多叉树，运算速度慢，CART为二叉树，运算速度快
2. C4.5只能分类，CART既可以分类也可以回归
3. CART使用Gini系数作为变量的不纯度量，减少了大量的对数运算
4. CART采用代理测试来估计缺失值，而C4.5以不同概率划分到不同节点中
5. CART采用“基于代价复杂度剪枝”方法进行剪枝，而C4.5采用悲观剪枝方法
~~~~
- 基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于0~1之间的数，0是完全相等，1是完全不相等
- CART对于缺失值的处理：
~~~~
1. 模型对于缺失值的处理会分为两个子问题：
- 在特征值缺失的情况下进行划分特征的选择？
- 选定该划分特征，对于缺失该特征值的样本如何处理？
2. 解答
- 对于问题1，CART一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的20%的记录是缺失的，那么这个特征就会减少20%或者其他数值
- 对于问题2，CART算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当CART树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。
~~~~
--- 

**2.3 剪枝：**
- 为什么要剪枝：过拟合的树在泛化能力的表现非常差，故通过主动去掉一些分支来降低过拟合的风险
- 预剪枝：在节点划分前来确定是否继续增长，及早停止增长的主要方法有：
~~~~
1. 节点内数据样本低于某一阈值
2. 所有节点特征都已分裂
3. 节点划分前准确率比划分后准确率高
4. 预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险
5. 预剪枝通过限制树的深度，叶子节点个数，叶子节点样本数，信息增益量等来实现
~~~~
- 后剪枝：在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树
~~~~
1. C4.5采用的悲观剪枝方法，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5通过训练数据集上的错误分类数量来估算未知样本上的错误率
2. 后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树；但同时其训练时间会大的多
~~~~
- CART算法中的后剪枝采用一种“基于代价复杂度的剪枝”方法进行，通过一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。代价复杂度剪枝算法为
```math
C_{\alpha}(T) = C(T)+\alpha|T|
```
其中T为任意子树，C(T)为预测误差，|T|为子树T的叶子节点个数，α是参数，C(T)衡量训练数据的拟合程度，|T|衡量树的复杂度，α权衡拟合程度与树的复杂度。
- CART的一大优势在于：无论训练数据集有多失衡，它都可以将其自动消除不需要建模人员采取其他操作
---

### 3、ID3，C4.5和CART算法对比
除了上文中列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，还有以下差异：
1. 划分标准的差异：ID3使用信息增益偏向特征值多的特征，C4.5使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART使用基尼指数克服C4.5需要求log的巨大计算量，偏向于特征值较多的特征
2. 使用场景的差异：ID3和C4.5都只能用于分类问题，CART可以用于分类和回归问题；ID3和C4.5是多叉树，速度较慢，CART是二叉树，计算速度很快
3. 样本数据的差异：ID3只能处理离散数据且缺失值敏感，C4.5和CART可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议C4.5、大样本建议CART。C4.5处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而CART本身是一种大样本的统计方法，小样本处理下泛化 误差较大
4. 样本特征的差异：ID3和C4.5层级之间只使用一次特征，CART可多次重复使用特征
5. 剪枝策略的差异：ID3没有剪枝策略，C4.5是通过悲观剪枝策略来修正树的准确性，而CART是通过代价复杂度剪枝。



**参考教程：**
1. 周志华--机器学习
2. 李  航--统计学习方法
3. [总结教程](https://www.kesci.com/home/project/5e7c836a98d4a8002d2dd54c)




