### 数据降维
---

#### 1.降维的概念
- 降维是按照一定的数学变换方法，将给定的一组特征通过模型从高维空间的数据点映射到低维度空间，然后利用映射后的变量特征来表示原有变量的总体特征
- 保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的
- 常见的降维包括：独立成分分析ICA、主成分分析PCA、因子分析FA、线性判别分析LDA、局部线性嵌入LLE、核主成分分析Kernel PCA等

#### 2.降维的优点
1. 使得数据集更易使用
2. 降低算法的计算开销
3. 去除噪声
4. 使得结果容易理解

#### 3.降维的方法
##### 3.1 PCA--主成分分析
PCA是一种使用最广泛的数据降维算法，PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征

**PCA逻辑**
1. 从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。
2. 其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴

**PCA有两种通俗易懂的解释：**
1. 最大方差理论
2. 最小化降维造成的损失

**如何得到包含最大差异性的主成分方向**
1. 事实上，通过计算数据矩阵的协方差矩阵
2. 然后得到协方差矩阵的特征值特征向量
3. 选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵
4. 这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维

**PCA主要应用场景**
1. 非监督式类型的数据集，作为非监督式降维方法，适用不带标签的数据集，带有标签的可以采用LDA
2. 根据方差自主控制特征数量
3. 更少的正则化处理，选择较多的主成分将导致较少的平滑，因为PCA能够保留更多的数据特征，从而减少正则化
4. 数据量较大的数据集，可以提升数据处理效率
5. 数据分布是位于相同平面上(非曲面)，数据中存在线性结构

##### 3.2 FA--因子分析
1. 因子分子指研究从变量群中提取共性因子的统计技术，这里的共性因子指不同变量之间内在的隐藏因子，因此因子分子的过程是寻找共性因子和个性因子，并得到最有解释的过程
2. 因子分析的目的在于用更少的、未观测到的变量（factor）描述观测到的、相关的变量
3. 更准确的来说，因子分析假设在观测到的变量间存在某种相关关系，从观测变量的矩阵内部的相关关系出发找到潜变量(latent variables)，从而使得潜变量和观测变量之间的关系成立

**PCA和FA的异同点：**
**- 相同点：**
1. 二者都是数据降维的方法，对原始数据做标准化处理，消除原始指标的相关性对综合评价所造成的信息重复的影响
2. 二者构造综合评价时所涉及的权数具有客观性，在原始信息损失不大的前提下，减少了后期数据挖掘和分析的工作量
3. 二者的侧重点都是进行数据降维，因此很少单独使用，大多数情况下都会和一些模型组合使用；但由于对数据维度进行一定的处理，处理的结果都偏离了原有维度的认识。
- **差异点：**
1. 原理不同，PCA的逻辑是基于变量的线性变换组合，在损失较少信息的前提下将多指标转换为几个不相关的主成分，每个主成分都是原始变量的想线性组合；因子分析是基于因子的组合，从原始变量相关矩阵内部的依赖关系出发，通过少数共性因子和个性因子来展现原有的原始变量。因子分析是主成分分析的推广，更倾向于描述原始变量之间的相关关系
2. 假设条件不同，PCA不需要假设，因子分析需要假设各个共同因子之间不相关，特殊因子之间也不相关，共性因子和特殊因子之间也不相关
3. 求解方法不同，PCA从协方差矩阵出发，因子分析包括主成分法、主轴因子法、极大似然法、最小二乘法、a因子提取法
4. 降维后的维度数量不同，即因子数量和主成分数量

##### 3.3 LDA--线性判别分析
判别分析是一种分类方法，通过一个已知类别的训练样本建立判别标准，并通过预测变量来为未知类别的数据进行分类。LDA是其中一种，也是模式识别的经典算法。

**LDA的逻辑**：将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性

**PCA与LDA相比的区别：**
- 出发思想不同，PCA主要从特征的协方差出发，找到方差最大方向的投影；LDA主要考虑分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向
- 学习模式不同，PCA属于无监督学习，常与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此可以独立使用
- 降维后可用维度数量不同，LDA降维后最多可生成C-1维子空间(分类标签数-1)，因此LDA与原始维度数量无关，只与数据标签分类数量有关；PCA最多只有n维度可用，即最大可以选择全部可用维度

**LDA算法的局限性**
1. 当样本量远小于样本的特征维数时，样本之间的距离变大使得距离度量失控，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现尤为突出
2. LDA不适合对非高斯分布的样本进行降维
3. LDA在样本分类信息依赖方差而不是均值时，效果不好
4. LDA可能会出现过拟合现象

#### 4.基于特征组合的降维
基于特征组合的降维是将输入特征与目标预测变量做拟合的过程；输入变量经过运算并能得到对目标变量做出很好解释(预测性)的复合特征，这些复合特征时经过组合变换后的新特征。

**优点**
- 在一定程度上解决了单一特征的离散和稀疏的问题，新组合特征对目标变量的解释能力增加
- 降低原有特征中的噪音信息的干扰，使得模型鲁棒性更强
- 降低了模型的复杂程度并提高模型效率
- 如果将原有特征与组合特征共同加入训练集汇总，能有效兼顾全局特征(原有单一特征表达的信息)和个性化特征(新组合特征表达的信息)，能提高准确率

**特征组合的方法**
1. 基于单一特征离散化后的组合，该方法先将连续性特征离散化，然后基于离散化后的特征组合成新的特征，例如RFM模型
2. 基于单一特征的运算后的组合，该方法对单一列基于不同条件下获得的数据记录做求和、均值、最大值等，从而获得新特征
3. 基于多个特征的运算后的组合，对多个单一特征做复合计算
4. 基于模型的特征最优组合

[特征转换的代码](https://github.com/isidore-wong/MachineLearning-Algorithm/tree/master/00_DataProcessing%26FeatureEngineering)