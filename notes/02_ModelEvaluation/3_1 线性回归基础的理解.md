## 线性回归(Linear Regression)
---

### 1、线性回归公式
给定数据集D，通过线性回归拟合出一个线性模型，尽可能的准确预测实值输出标记。其中数据集D为：
```math
D = \{(x_1, y_1),(x_2, y_2),..., (x_n, y_n)\}
```

预测函数为：
```math
f(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n

=w^T x
```

由于预测值域真实值之间肯定存在差异，用$\epsilon$来表示，则函数为：
```math
f(x_i) = w_i x_i + \epsilon_i  \ \ \ \ \ \ \ \ \ \ \ \ \  \ (1)
```

其中 $\epsilon_i$ 是独立并且具有相同的分布，并且服从均值为0方差为 $\theta^2$ 的高斯分布.
```math
p(\epsilon_i) = {\frac{1}{\sqrt{2\pi\sigma}} exp{(-\frac{(\epsilon_i)^2}{2\sigma^2})}}
\ \ \ \ \ \ \ \ \ (2)
```

在模型评估中我们知道**均方误差是回归任务重最常用的性能度量**，因此我们可以基于均方误差最小化来使得 $\epsilon_i$ 最小化，而基于均方误差最小化进行模型求解的方法被称为**最小二乘法**，使所有样本到拟合直线上的欧式距离之和最小，公式如下：
```math
argmin \sum_{i=1}^{n}{(f(x_i)-y_i)^2}
\ \ \ \ \ \ \ \ (3)
```

结合公式(1)(2)(3)后，求和使误差最小的 $w$, 
```math
p(y_i|x_i;w) = {\frac{1}{\sqrt{2\pi\sigma}} exp{(-\frac{(y_i-w^T x_i)^2}{2\sigma^2})}}
```

从而求得似然函数为：
```math
L(\theta) = \prod_{i=1}^n p(y_i|x_i;w)
= \prod_{i=1}^n {\frac{1}{\sqrt{2\pi\sigma}} exp{(-\frac{(y_i-w^T x_i)^2}{2\sigma^2})}}
```

上式中由于是乘法难解，对上式求对数变为加法
```math
log{L(\theta)} = log{\prod_{i=1}^n {\frac{1}{\sqrt{2\pi\sigma}} exp{(-\frac{(y_i-w^T x_i)^2}{2\sigma^2})}}}

= \sum_{i=1}^{n} log{{\sqrt{2\pi\sigma}} exp{(-\frac{(y_i-w^T x_i)^2}{2\sigma^2})}}

= n log \frac{1}{\sqrt{2\pi\sigma}} - \frac{1}{\sigma^2} * \frac{1}{2}\sum_{i=1}^{n}(y_i - w^T x_i)^2
```

我们的目标是：让似然函数(对数变换后也一样)越大越好，这样误差才会越小，故采用最小二乘法求得
```math
J(w) = \frac{1}{2} \sum_{i=1}^{n} (y_i - w^T x_i)^2

= \frac{1}{2} \sum_{i=1}^{n} (f_w(x) - y_i)^2

= \frac{1}{2} (Xw - y)^T (Xw - y)
```

对w求偏导得到：
```math
\frac{\partial J(w)}{\partial w}
= \nabla_w J(w)

= \nabla_w (\frac{1}{2} (Xw - y)^T (Xw - y))

=\nabla_w (\frac{1}{2} (X^T w^T - y^T) (Xw - y))

=\nabla_w (\frac{1}{2} (X^T w^T Xw - y^T Xw - X^T w^T y + y^T y))

=\frac{1}{2} (2X^T Xw - X^T y -(y^T X)^T)

= X^T Xw - X^T y
```
令偏导等于0：
```math
w = (X^T X)^{-1} X^T y
```

**似然函数：** 是一种关于统计模型中的参数的函数，表示模型参数中的似然性。通俗理解就是在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计，根据样本估计参数值。

**概率：**
用于在已知一些参数的情况下，预测接下来的观测所得到的结果。

### 2、模型评估方法
常用的评估项为 $R^2$，均方误差等，下面介绍 $R^2$：
```math
R^2 = 1 - \frac{\sum_{i=1}^{n}(\hat{y_i} - y_i)^2}{\sum_{i=1}^{n}(y_i -\overline{y_i})^2}
```
其中分子为残差项，分母为类似方差项，$R^2$ 的取值越接近1我们认为模型拟合的越好。

**Show me the code** 

[最小二乘法的学习与理解](https://github.com/isidore-wong/MachineLearning-Algorithm/blob/master/code/02_Algorithm/least_sqaure_method.ipynb/)

[线性回归实现代码](https://github.com/isidore-wong/MachineLearning-Algorithm/blob/master/code/02_Algorithm/linear_regression.py)



**参考教程**
> [1] 周志华 机器学习

