## 模型评估指标
---

### 1、性能度量
采用有效可行的实验估计方法，根据衡量模型泛化能力的评价标准对学习器的泛化性能进行评估。

**例子：**
对给定样本集进行预测，其中$y_i$是示例$x_i$的真实标记，要评估学习器f的性能，把预测结果与真实标记进行比较。

其中样本集D为：
```math
D = \{{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)}\}
```

回归任务中最常用的性能度量是“均方误差(mean squared error)”
```math
E(f;D)=\frac{1}{m} \sum_{i=1}^{m}(f(x_i)-y_i)^2
```

更一般的，对于数据分布D和概率密度函数p，均方误差可描述为：
```math
E(f;D)=\int_{x}^{D}{(f(x)-y)^2}{p(x)}\text{d}x
```
**均方误差的开方--RMSE**却会产生 **“意外”**
```math
RMSE = \sqrt{\frac{\sum_{i=1}^{m}(f(x_i)-y_i)^2}{m} }
```
~~~~
产生意外的原因：
1. 一般情况下，RMSE能够很好的反映回归模型预测值与真实值偏离的程度；
2. 在实际问题中，如果存在个别偏离程度非常大的离群点时，集市离群点数量非常少，也会让RMSE指标变得很差；原因可能是由于其他的5%区间内存在非常严重的离群点
3. 针对该问题的解决方案：
  -   如果认定这些离群点是“噪声点” 的话，就需要在数据预处理阶段把噪声点过滤掉
  -   如果不认为这些离群点是“噪声点”的话，就需要进一步提高模型的预测能力
  -   可以找一个更合适的指标来评估该模型，比如平均绝对百分比误差(Mean Absolute Percent Error, MAPE)
~~~~
**平均绝对百分比误差(Mean Absolute Percent Error, MAPE)** 相当于把每个点的误差进行归一化，降低了个别离群点带来的绝对误差的影响。
```math
MAPE = \sum_{i=1}^{m} \vert{\frac{(y_i - f(x_i))}{y_i}} \vert \ \times {\frac{100}{m}}
```

#### 1.1、错误率和精度
这两个指标是分类任务重最常用的性能指标。
- **错误率：** 分类错误的样本数占总样本数的比例
- **精度：** 分类正确的样本数占总样本数的比例

#### 1.2、查准率、查全率与F1
错误率和精度虽常用，但并不能满足所有的任务需求；而“查准率/精确率(precision)”和“查全率/召回率(Recall)”更加适合二分类的需求。

**精确率和召回率**
对于二分类问题，根据样本的真实类别和学习器预测类别的组合可划分为以下几类：
真实情况 | 预测结果(正例) | 预测结果(反例) |
---|--- |---|
正例 | TP(真正例) |FN(假反例) |
反例 | FP(假正例) | TN(真反例) |

**精确率P和召回率R**分别定义为：
```math
P = \frac{TP}{TP+FP}
```

```math
R = \frac{TP}{TP+FN}
```

精确率precision和召回率recall是既矛盾又统一的两个指标，一般来说precision高时，recall较低（为了提高precision值，学习器需要尽量在“更有把握”时才把样本预测为正，但往往由于过于保守而漏掉很多“没有把握”的正样本，导致recall值降低）；而recall高时，precision较低。

**F1 score**：为了平衡精确率和召回率，需要一个性能指标综合反映，F1是基于精确率和召回率的调和平均。
```math
F1 = \frac{2 \times precision \times recall}{ precision + recall}
= \frac{2 \times TP}{样例总数 + TP -TN}
```

```math
\frac{1}{F1} = \frac{1}{2} * ({\frac{1}{P}} + {\frac{1}{R}})
```

在一些应用中，对精确率和召回率的重视程度有所不同，因此F1度量会进行加权调和平均
```math
F_\beta = \frac{(1+\beta)^2 \times P \times R}{(\beta^2 \times P) + R}
```
通过$\beta$值的改变来调整精确率和召回率的侧重性，$\beta = 1$时退化为标准的F1，$\beta > 1$时召回率有更大影响，$\beta < 1$时精确率有更大的影响。

#### 1.3、ROC与AUC
**ROC曲线**
ROC曲线(Receiver Operating Characteristic Curve)--受试者工作特征曲线。

ROC曲线的横坐标为假正例率(False Positive Rate, FPR)，纵坐标为真正例率(True Positive Rate, TPR)，在ROC曲线上的点坐标为(FPR, TPR)
```math
TPR = \frac{TP}{P} = \frac{TP}{TP+FN}

FPR = \frac{FP}{N} = \frac{FP}{FP+TN}
```

**绘制ROC曲线**
ROC曲线是通过不断移动学习器的“截断点”来生成曲线上的一组关键点的，截断点指的是区分正负预测结果的阈值。

比较直观的绘制ROC曲线：
1. 首先根据标签统计出正负样本的数量，假设正样本数量为P，负样本为N；
2. 把横轴的刻度间隔设置为1/N，纵轴刻度间隔设置为1/P；
3. 根据模型输出的预测概率对样本进行排序(从高到低)
；
4. 依次遍历样本，同时从原点开始绘制ROC曲线，知道遍历完所有样本，曲线最终停在(1, 1)这个点。

**如何计算AUC：**

AUC(Area Under Curve)是ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。

由于ROC曲线一般都处于y=x这条直线的上方(如果不是的话，只要把模型预测的概率反转成1-p就可以得到一个更好的分类器)，所以AUC的取值一般在0.5 ~ 1之间。

AUC越大，说明学习器越可能把真正的正样本排在前面，分类性能越好。



