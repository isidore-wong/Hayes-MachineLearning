### 数据共线性问题
---

#### 1.共线性
共线性(多重共线性)指定是输入的自变量之间存在较高的线性相关度，会导致回归模型的稳定性和准确性大大降低，且过多无关维度参与计算也会浪费计算资源和时间

#### 2.具有明显的共线性维度或变量的业务场景有：
- 访问量与页面的浏览量
- 页面浏览量与访问时间
- 订单量与销售额
- 订单量与转化率
- 促销费用与销售额
- 网络广告费用与访客数

#### 3.变量间出现共线性的原因：
- 数据样本不够，导致共线性存在偶然性
- 多个变量都基于时间，有共同或相反的演变趋势
- 多个变量间存在一定的推移关系，但总体上变量间的趋势一致，只是发生的时间点不一致
- 多个变量间存在近似线性的关系

#### 4.如何检验共线性
共线性一般通过容忍度、方差膨胀因子、特征值等几个特征数据来做检验
- 容忍度：是每个自变量作为因为边对其他自变量进行归回建模时得到的残差比例，容忍度值介于0-1之间，值越小说明共线性问题的可能性越大
- 方差膨胀因子VIF：VIF是容忍度的倒数，值越大则共线性问题越明显，通常以10作为判断标准，VIF<10代表不存在多重共线性，10<=VIF<100代表存在较强的多重共线性，VIF>=100代表存在严重多重共线性
- 特征值：实际上就是对自变量进行主成分分析，如果多个维度的特征值等于0，则可能存在较严重的共线性

#### 5.解决共线性的常用方法
1. 增大样本量：通过增加样本量来消除由于数据量不足而出现的偶然共线性现象
2. 岭回归法：是一种专用于共线性问题的有偏估计回归方法，实质上是一种改良的最小二乘估计法；通过房企最小二乘法的无偏性，以损失部分信息、降低精度为代价来获得更实际和更可靠的回归系数
3. 逐步回归法：每次引入一个自变量并进行统计检验，然后逐步引入其他变量，同时对所有变量的回归系数进行检验
4. 主成分回归：将原始参与建模的变量转换为少数几个主成分，并基于主成分做回归分析，在不丢失重要数据特征的前提下避开共线性问题
5. 人工去除：结合人工经验，对参与回归模型计算的自变量进行删减


[Show me the code](https://github.com/isidore-wong/MachineLearning-Algorithm/tree/master/00_DataProcessing%26FeatureEngineering)